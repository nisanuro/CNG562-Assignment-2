{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNG562-Assignment2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nisanuro/CNG562-Assignment-2/blob/master/CNG562_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "738YGK5FqWmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics, datasets, preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.naive_bayes import GaussianNB, BernoulliNB, CategoricalNB, ComplementNB, MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlXDAHx2Gt_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataVisualizaion(iris):\n",
        "    x_index = 0\n",
        "    y_index = 1\n",
        "\n",
        "    formatter = plt.FuncFormatter(lambda i, *args: iris.target_names[int(i)])\n",
        "    \n",
        "    plt.figure(figsize=(5, 4))\n",
        "    plt.scatter(iris.data[:, x_index], iris.data[:, y_index], c=iris.target)\n",
        "    plt.colorbar(ticks=[0, 1, 2], format=formatter)\n",
        "    plt.xlabel(iris.feature_names[x_index])\n",
        "    plt.ylabel(iris.feature_names[y_index])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Feo4-PRyTbvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def threeDVisualization(X, y):\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    fig = plt.figure(1, figsize=(16, 9))\n",
        "    ax = Axes3D(fig, elev=-150, azim=110)\n",
        "    X_reduced = PCA(n_components=3).fit_transform(X_scaled)\n",
        "    ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y, cmap=plt.cm.Set1, edgecolor='k', s=40)\n",
        "    ax.set_title(\"First three PCA directions\")\n",
        "    ax.set_xlabel(\"1st eigenvector\")\n",
        "    ax.w_xaxis.set_ticklabels([])\n",
        "    ax.set_ylabel(\"2nd eigenvector\")\n",
        "    ax.w_yaxis.set_ticklabels([])\n",
        "    ax.set_zlabel(\"3rd eigenvector\")\n",
        "    ax.w_zaxis.set_ticklabels([])\n",
        "\n",
        "    plt.show()\n",
        "    print(\"The number of features in the new subspace is \", X_reduced.shape[1])\n",
        "\n",
        "    return X_reduced"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skeQ9JV9_rlO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def randomOneHoldout(X_train, Y_train):\n",
        "    x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.2, random_state=0)\n",
        "    return x_train, x_test, y_train, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYjpybV6_v7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stratifiedOneHoldout(X_train, Y_train):\n",
        "    x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.2, random_state=0)\n",
        "    return x_train, x_test, y_train, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGpMxh6aK-FS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def NaiveBayes(X, Y):\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=9)\n",
        "    \n",
        "    # Gaussian Naive Bayes\n",
        "    gaussian = GaussianNB()\n",
        "    NaiveBayesValidations(X_train, Y_train, gaussian)\n",
        "\n",
        "    y_pred = gaussian.predict(X_test)\n",
        "    print(\"Accuracy - unseen data: \", metrics.accuracy_score(Y_test, y_pred)*100)\n",
        "    \n",
        "    fourError(X, Y, gaussian)\n",
        "    \n",
        "    \n",
        "    # Multinomial Naive Bayes\n",
        "    multinomial = MultinomialNB(fit_prior=True)\n",
        "    NaiveBayesValidations(X_train, Y_train, multinomial)\n",
        "\n",
        "    y_pred = multinomial.predict(X_test)\n",
        "    print(\"Accuracy - unseen data: \", metrics.accuracy_score(Y_test, y_pred)*100)\n",
        "\n",
        "    fourError(X, Y, multinomial)\n",
        "    \n",
        "    \n",
        "    # Bernoulli Naive Bayes\n",
        "    \n",
        "    bernoulli = BernoulliNB(binarize = 1.75)\n",
        "    bernoulli.fit(X_train, Y_train)\n",
        "    #NaiveBayesValidations(X_train, Y_train, bernoulli)\n",
        "\n",
        "    \n",
        "    y_pred = bernoulli.predict(X_test)\n",
        "    print(\"Accuracy - unseen data: \", metrics.accuracy_score(Y_test, y_pred)*100)\n",
        "\n",
        "    fourError(X, Y, bernoulli)\n",
        "    \n",
        "    #BernoulliBinarize(X_train, Y_train)\n",
        "    \n",
        "    \n",
        "    # Complement Naive Bayes\n",
        "    \n",
        "    complement = ComplementNB()\n",
        "    NaiveBayesValidations(X_train, Y_train, complement)\n",
        "\n",
        "    y_pred = complement.predict(X_test)\n",
        "    print(\"Accuracy - unseen data: \", metrics.accuracy_score(Y_test, y_pred)*100)\n",
        "\n",
        "    fourError(X, Y, complement)\n",
        "    \n",
        "    #complementTuning(X_train, Y_train)\n",
        "    \n",
        "    # Categorical Naive Bayes\n",
        "    categorical = CategoricalNB(fit_prior = False)\n",
        "    NaiveBayesValidations(X_train, Y_train, categorical)                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y2DlvqILDQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def complementTuning(X_train, Y_train):\n",
        "    fit_prior = [True, False]\n",
        "    norm = [True, False]\n",
        "\n",
        "    for i in fit_prior:\n",
        "        for j in norm:\n",
        "            model = ComplementNB(fit_prior = i, norm = j)\n",
        "\n",
        "            cv = cross_val_score(model, X_train, Y_train, cv=10, scoring='accuracy')\n",
        "\n",
        "            print(\"fit_prior = \", i, \"   norm = \", j, \"   Accuracy: \", cv.mean()*100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRiagr3DLGFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def BernoulliBinarize(X_train, Y_train):\n",
        "    binarize = [0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2, 2.25, 2.5, 2.75, 3]\n",
        "\n",
        "    for i in binarize:\n",
        "\n",
        "        model = BernoulliNB(binarize = i)\n",
        "        cv = cross_val_score(model, X_train, Y_train, cv=5, scoring='accuracy')\n",
        "        print(\"Binarize = \", i, \"    Accuracy: \", cv.mean()*100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdOwVmd3LJkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def NaiveBayesValidations(X_train, Y_train, model):\n",
        "    print(\"\\n\" + str(model).split('N')[0] + \" Naive Bayes Accuracy\\n\")\n",
        "    # 5-Fold\n",
        "    cv = cross_val_score(model, X_train, Y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "    print(\"5-Fold: \", cv.mean()*100)\n",
        "    \n",
        "    # 10-Fold\n",
        "    cv = cross_val_score(model, X_train, Y_train, cv=10, scoring='accuracy')\n",
        "\n",
        "    print(\"10-Fold: \", cv.mean()*100)\n",
        "\n",
        "    # Random One Holdout\n",
        "    x_train, x_test, y_train, y_test = randomOneHoldout(X_train, Y_train)\n",
        "    \n",
        "    model.fit(x_train, y_train)\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    print(\"Random One Holdout: \", metrics.accuracy_score(y_test, y_pred)*100)\n",
        "\n",
        "    # Stratified One Holdout\n",
        "    x_train, x_test, y_train, y_test = stratifiedOneHoldout(X_train, Y_train)\n",
        "\n",
        "    model.fit(x_train, y_train)\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    print(\"Stratified One Holdout: \", metrics.accuracy_score(y_test, y_pred)*100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwb5WKRtLNGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def DecisionTree(X, Y):\n",
        "    \n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "    \n",
        "    # 5-Fold\n",
        "    print(\"\\n5-Fold: \")\n",
        "    tuningDepth(X_train, Y_train, 0)\n",
        "\n",
        "    # 10-Fold\n",
        "    print(\"\\n10-Fold: \")\n",
        "    tuningDepth(X_train, Y_train, 1)\n",
        "\n",
        "    # Random One Holdout\n",
        "    print(\"\\nRandom One Holdout: \")   \n",
        "    tuningDepth(X_train, Y_train, 2)\n",
        "  \n",
        "    # Stratified One Holdout\n",
        "    print(\"\\nStratified One Holdout: \")\n",
        "    tuningDepth(X_train, Y_train, 3)\n",
        "    \n",
        "    #\n",
        "    #   Continue with 10-Fold, Depth = 3\n",
        "    #\n",
        "    \n",
        "    print(\"5-Fold, Depth=5\\n\")\n",
        "    tuningSplit(X_train, Y_train)\n",
        "    \n",
        "    #\n",
        "    #   Continue with criterion = 'gini', splitter = 'best', min_samples_split = 2\n",
        "    #                     all of them are default values\n",
        "    print(\"5-Fold, depth = 5, criterion = 'gini', splitter = 'best, min_samples_split = 2\\n\")\n",
        "    tuningClassWeight(X_train, Y_train)\n",
        "\n",
        "    #\n",
        "    #   Continue with class_weight = None,   default\n",
        "    #\n",
        "    \n",
        "    clf = DecisionTreeClassifier(max_depth = 3)\n",
        "    '''\n",
        "    clf.fit(X_train, Y_train)\n",
        "    \n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(\"Accuracy: \", metrics.accuracy_score(Y_test, y_pred)*100)\n",
        "    '''\n",
        "    fourError(X, Y, clf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UicrTaLkLRSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tuningClassWeight(X_train, Y_train):\n",
        "    # No class weight\n",
        "    clf = DecisionTreeClassifier(max_depth = 3, random_state = 0)\n",
        "    clf.fit(X_train, Y_train)\n",
        "\n",
        "    cv = cross_val_score(clf, X_train, Y_train, cv=10, scoring='accuracy')\n",
        "    print(\"Class weight: None           Accuracy: \", cv.mean()*100)\n",
        "\n",
        "    # Balanced class weight\n",
        "    clf = DecisionTreeClassifier(max_depth = 3, random_state = 0, class_weight ='balanced')\n",
        "    clf.fit(X_train, Y_train)\n",
        "\n",
        "    cv = cross_val_score(clf, X_train, Y_train, cv=10, scoring='accuracy')\n",
        "    print(\"Class weight: Balanced       Accuracy: \", cv.mean()*100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-J2S6OKkLUrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tuningSplit(X_train, Y_train):\n",
        "    criterion = [\"gini\", \"entropy\"]\n",
        "    splitter = [\"best\", \"random\"]\n",
        "\n",
        "    for i in criterion:\n",
        "        for j in splitter:\n",
        "            clf = DecisionTreeClassifier(criterion = i, splitter = j, max_depth = 3, random_state = 0)\n",
        "            clf.fit(X_train, Y_train)\n",
        "\n",
        "            cv = cross_val_score(clf, X_train, Y_train, cv=10, scoring='accuracy')\n",
        "            print(\"Criterion: \", i, \"   Splitter: \", j, \"   Accuracy: \", cv.mean()*100)\n",
        "\n",
        "    for i in range(2, 10):\n",
        "        clf = DecisionTreeClassifier(max_depth = 3, min_samples_split = i, random_state = 0)\n",
        "        clf.fit(X_train, Y_train)\n",
        "\n",
        "        cv = cross_val_score(clf, X_train, Y_train, cv=10, scoring='accuracy')\n",
        "        print(\"min_samples_split: \", i, \"   Accuracy: \", cv.mean()*100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JhAsgk2LZAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tuningDepth(X_train, Y_train, val):\n",
        "\n",
        "    max_depth_range = list(range(1, 10))\n",
        "    max_depth_range.append(str(\"None\"))\n",
        "    \n",
        "\n",
        "    for depth in max_depth_range:\n",
        "        if (val == 0):\n",
        "            if(depth == \"None\"):\n",
        "                clf = DecisionTreeClassifier(random_state = 0)\n",
        "                clf.fit(X_train, Y_train)\n",
        "            else:               \n",
        "                clf = DecisionTreeClassifier(max_depth = depth, random_state = 0)\n",
        "                clf.fit(X_train, Y_train)\n",
        "\n",
        "            cv = cross_val_score(clf, X_train, Y_train, cv=5, scoring='accuracy')\n",
        "            accuracy = cv.mean()*100\n",
        "            print(\"Depth: \", depth, \" Accuracy: \", accuracy)\n",
        "            \n",
        "        elif (val == 1):\n",
        "            if(depth == \"None\"):\n",
        "                clf = DecisionTreeClassifier(random_state = 0)\n",
        "                clf.fit(X_train, Y_train)\n",
        "            else:  \n",
        "                clf = DecisionTreeClassifier(max_depth = depth, random_state = 0)\n",
        "                clf.fit(X_train, Y_train)\n",
        "\n",
        "            cv = cross_val_score(clf, X_train, Y_train, cv=10, scoring='accuracy')\n",
        "            accuracy = cv.mean()*100\n",
        "            print(\"Depth: \", depth, \" Accuracy: \",accuracy)\n",
        "            \n",
        "        elif(val == 2):            \n",
        "            x_train, x_test, y_train, y_test = randomOneHoldout(X_train, Y_train)\n",
        "\n",
        "            if(depth == \"None\"):\n",
        "                clf = DecisionTreeClassifier(random_state = 0)\n",
        "                clf.fit(X_train, Y_train)\n",
        "            else:  \n",
        "                clf = DecisionTreeClassifier(max_depth = depth, random_state = 0)\n",
        "                clf.fit(x_train, y_train)\n",
        "\n",
        "            accuracy = clf.score(x_test, y_test)*100\n",
        "            print(\"Depth: \", depth, \" Accuracy: \", accuracy)\n",
        "            \n",
        "        elif(val == 3):\n",
        "            x_train, x_test, y_train, y_test = stratifiedOneHoldout(X_train, Y_train)\n",
        "\n",
        "            if(depth == \"None\"):\n",
        "                clf = DecisionTreeClassifier(random_state = 0)\n",
        "                clf.fit(X_train, Y_train)\n",
        "            else:  \n",
        "                clf = DecisionTreeClassifier(max_depth = depth, random_state = 0)\n",
        "                clf.fit(x_train, y_train)\n",
        "\n",
        "            accuracy = clf.score(x_test, y_test)*100\n",
        "            print(\"Depth: \", depth, \" Accuracy: \", accuracy)\n",
        "            \n",
        "        else:\n",
        "            print(\"Invalid validation tech.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BVBws30S0w7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def kNN(k: int, metric: str, X_train, Y_train):\n",
        "  \n",
        "    #Model\n",
        "    if metric == \"mahalanobis\":\n",
        "      knn = KNeighborsClassifier(n_neighbors=k, weights='distance', metric=metric, algorithm=\"brute\", metric_params={'V': np.cov(X_train)})\n",
        "    else:\n",
        "      knn = KNeighborsClassifier(n_neighbors=k, weights='distance', metric=metric)\n",
        "\n",
        "    #5-Fold\n",
        "    cv_result_knn_5 = cross_val_score(knn, X_train, Y_train, cv=5, scoring='accuracy')\n",
        "  \n",
        "    #10-Fold\n",
        "    cv_result_knn_10 = cross_val_score(knn, X_train, Y_train, cv=10, scoring='accuracy')\n",
        "  \n",
        "    #Random One Holdout\n",
        "    x_train, x_test, y_train, y_test_random = randomOneHoldout(X_train, Y_train)\n",
        "    knn.fit(x_train, y_train)\n",
        "\n",
        "    y_pred_knn_random = knn.predict(x_test)\n",
        "  \n",
        "    #Stratified One Holdout\n",
        "    x_train, x_test, y_train, y_test_stratified = stratifiedOneHoldout(X_train, Y_train)\n",
        "    knn.fit(x_train, y_train)\n",
        "    y_pred_knn_stratified = knn.predict(x_test)\n",
        "\n",
        "    print(\"5 Fold\")\n",
        "    print(\"KNN Accuracy: \", cv_result_knn_5.mean())\n",
        "  \n",
        "    print(\"10 Fold\")\n",
        "    print(\"KNN Accuracy: \", cv_result_knn_10.mean())\n",
        "\n",
        "    print(\"Random One Hold Out\")\n",
        "    print(\"KNN Accuracy: \", 1 - metrics.mean_squared_error(y_test_random, y_pred_knn_random))\n",
        "  \n",
        "    print(\"Stratified One Hold Out Fold\")\n",
        "    print(\"KNN Accuracy: \", 1 - metrics.mean_squared_error(y_test_stratified, y_pred_knn_stratified))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k3k_9eDQ6t9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def svm(X_train, Y_train, kernel, weight, gamma):\n",
        "\n",
        "    svm = SVC(C=1, kernel=kernel, degree=3, gamma=gamma, coef0=0.0, shrinking=True, \n",
        "          probability=False, tol=0.001, cache_size=200, class_weight=weight,\n",
        "          max_iter=-1, decision_function_shape=\"ovr\", random_state = 0)\n",
        "\n",
        "    #5-Fold\n",
        "    cv_result_svm_5 = cross_val_score(svm, X_train, Y_train, cv=5, scoring='accuracy')\n",
        "  \n",
        "    #10-Fold\n",
        "    cv_result_svm_10 = cross_val_score(svm, X_train, Y_train, cv=10, scoring='accuracy')\n",
        "  \n",
        "    #Random One Holdout\n",
        "    x_train, x_test, y_train, y_test_random = randomOneHoldout(X_train, Y_train)\n",
        "    svm.fit(x_train, y_train)\n",
        "    y_pred_svm_random = svm.predict(x_test)\n",
        "  \n",
        "    #Stratified One Holdout\n",
        "    x_train, x_test, y_train, y_test_stratified = stratifiedOneHoldout(X_train, Y_train)\n",
        "    svm.fit(x_train, y_train)\n",
        "    y_pred_svm_stratified = svm.predict(x_test)\n",
        "\n",
        "    print(\"5 Fold\")\n",
        "    print(\"SVM Accuracy: \", cv_result_svm_5.mean())\n",
        "  \n",
        "    #print(\"10 Fold\")\n",
        "    #print(\"SVM Accuracy: \", cv_result_svm_10.mean())\n",
        "\n",
        "    #print(\"Random One Hold Out\")\n",
        "    #print(\"SVM Accuracy: \", 1 - metrics.mean_squared_error(y_test_random, y_pred_svm_random))\n",
        "  \n",
        "    #print(\"Stratified One Hold Out Fold\")\n",
        "    #print(\"SVM Accuracy: \", 1 - metrics.mean_squared_error(y_test_stratified, y_pred_svm_stratified))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaldHpB1LjQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def zValues(df):\n",
        "    \n",
        "    cols = list(df.columns)\n",
        "    cols.remove('Index')\n",
        "\n",
        "    for col in cols:\n",
        "        col_zscore = col + '_zscore'\n",
        "        df[col_zscore] = (df[col] - df[col].mean())/df[col].std(ddof=0)\n",
        "\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bgd_SgcOLlY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def outliers(df):  \n",
        "    return df.loc[(df._1_zscore > 2.5) | (df._2_zscore > 2.5) | (df._2_zscore > 2.5) | (df._3_zscore > 2.5)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c_Vbu1AW3LQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def subDatasets(df):\n",
        "    target0 = []\n",
        "    target1 = []\n",
        "    target2 = []\n",
        "    for row in df.itertuples():\n",
        "        if row.target == 0:\n",
        "            target0.append(row)\n",
        "            target0_df = pd.DataFrame(target0)\n",
        "        elif row.target == 1:\n",
        "            target1.append(row)\n",
        "            target1_df = pd.DataFrame(target1)\n",
        "        else:\n",
        "            target2.append(row)    \n",
        "            target2_df = pd.DataFrame(target2)  \n",
        "\n",
        "    dfs = [target0_df, target1_df, target2_df]  \n",
        "\n",
        "    for df in dfs:\n",
        "        df.drop(columns=['target'])\n",
        "    \n",
        "    return target0_df, target1_df, target2_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-bInRUkY-PB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def AdaBoost(model, n_estimators, learning_rate, X_train, Y_train, X_test, Y_test):\n",
        "    clf = AdaBoostClassifier(base_estimator = model, n_estimators= n_estimators, learning_rate=learning_rate, random_state=0)\n",
        "    clf.fit(X_train, Y_train)\n",
        "    clf.predict(X_test)\n",
        "    return clf.score(X_train, Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zD9wkpoZuj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GradientBoost(n_estimators, learning_rate, X_train, Y_train, X_test, Y_test):\n",
        "    clf = GradientBoostingClassifier(n_estimators= n_estimators, learning_rate=learning_rate, random_state=0)\n",
        "    clf.fit(X_train, Y_train)\n",
        "    clf.predict(X_test)\n",
        "    return clf.score(X_train, Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3ZtQKDStF5Sm",
        "colab": {}
      },
      "source": [
        "def fourError(X, Y, model):\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0, stratify=Y)\n",
        "    \n",
        "    Train_x, TrainDev_x, Train_y, TrainDev_y = train_test_split(X_train, Y_train, test_size=0.2, random_state=0, stratify=Y_train)\n",
        "    Dev_x, Test_x, Dev_y, Test_y = train_test_split(X_test, Y_test, test_size=0.5, random_state=0, stratify=Y_test)\n",
        "\n",
        "    model.fit(Train_x, Train_y)\n",
        "\n",
        "    y_true, trainDev_pred = TrainDev_y, model.predict(TrainDev_x)\n",
        "\n",
        "    print(\"Train-Train Dev,   e1:\", metrics.mean_squared_error(TrainDev_y, trainDev_pred),\"\\n\")\n",
        "    print(\"KNN Accuracy: \", 1 - metrics.mean_squared_error(TrainDev_y, trainDev_pred))\n",
        "    print( '\\nClassification report\\n' )\n",
        "    print(classification_report(y_true, trainDev_pred))\n",
        "\n",
        "    y_true, dev_pred = Dev_y, model.predict(Dev_x)\n",
        "    print(\"Train-Dev,   e2\", metrics.mean_squared_error(Dev_y, dev_pred),\"\\n\")\n",
        "    print(\"KNN Accuracy: \", 1 - metrics.mean_squared_error(Dev_y, dev_pred))\n",
        "    print( '\\nClassification report\\n' )\n",
        "    print(classification_report(y_true, dev_pred))\n",
        "\n",
        "    y_true, test_pred = Test_y, model.predict(Test_x)\n",
        "    print(\"Train-Test,   e3: \", metrics.mean_squared_error(Test_y, test_pred),\"\\n\")\n",
        "    print(\"KNN Accuracy: \", 1 - metrics.mean_squared_error(Test_y, test_pred))\n",
        "    print( '\\nClassification report\\n' )\n",
        "    print(classification_report(y_true, test_pred))\n",
        "\n",
        "    y_true, devTest_pred = Y_test, model.predict(X_test)\n",
        "    print(\"Train-(Dev+Test),   e4: \", metrics.mean_squared_error(Y_test, devTest_pred),\"\\n\")\n",
        "    print(\"KNN Accuracy: \", 1 - metrics.mean_squared_error(Y_test, devTest_pred))\n",
        "    print( '\\nClassification report\\n' )\n",
        "    print(classification_report(y_true, devTest_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGEz0duV_zJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def displayAccuracy(X, Y):\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "    k = [3, 5, 7, 9, 11]\n",
        "    metric = [\"euclidean\", \"manhattan\", \"chebyshev\", \"mahalanobis\", \"minkowski\", \"wminkowski\", \"seuclidean\"]\n",
        "    kernel = [\"linear\", \"rbf\"]\n",
        "    weight = [None, \"balanced\"]\n",
        "    gamma = [\"auto\", \"scale\"]\n",
        "\n",
        "    for i in k:\n",
        "      for j in metric:\n",
        "          if j != \"wminkowski\" and j != \"seuclidean\":\n",
        "            print(\"K: {} - Metric: {}\".format(i, j))\n",
        "            kNN(i, j, X_train, Y_train)\n",
        "          print()\n",
        "    \n",
        "    for i in kernel:\n",
        "        for j in weight:\n",
        "            for k in gamma:\n",
        "                if i != \"linear\":\n",
        "                    print(\"Kernel: {} - Weight: {} - Gamma: {}\".format(str(i), j, k))\n",
        "                    svm(X_train, Y_train, i, j, k)\n",
        "                else:\n",
        "                    print(\"Kernel: {} - Weight: {} - Gamma: {}\".format(str(i), j, \"auto\"))\n",
        "                    svm(X_train, Y_train, i, j, k)\n",
        "        print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er-z4_9i_223",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "  iris = datasets.load_iris()\n",
        "  X = iris.data\n",
        "  Y = iris.target\n",
        "  \n",
        "  #threeDVisualization(iris.data[:, :], Y)\n",
        "  \n",
        "  # Z-Score\n",
        "  scaler = StandardScaler()\n",
        "  scaler.fit(X)\n",
        "  z_score = scaler.transform(X)\n",
        "\n",
        "  #Displaying result according to each type of methods and regression model\n",
        "  print(\"\\nRaw: \")\n",
        "  displayAccuracy(X,Y)\n",
        "  #print(\"\\nZ-Score: \")\n",
        "  #displayAccuracy(z_score,Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yab09cbEcfMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    svm = SVC(C=1, degree=3, gamma=\"scale\", coef0=0.0, shrinking=True, \n",
        "          probability=True, tol=0.001, cache_size=200, class_weight=\"balanced\",\n",
        "          max_iter=-1, decision_function_shape=\"ovr\", random_state = 0)\n",
        "    \n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
        "    \n",
        "    #svm.fit(X_train, Y_train)\n",
        "    #y_true, y_pred = Y_test, svm.predict(X_test)\n",
        "    #print(\"SVM Accuracy: \", 1 - metrics.mean_squared_error(Y_test, y_pred))\n",
        "\n",
        "    fourError(X, Y, svm)\n",
        "\n",
        "    #AdaBoost(svm, 100, 1, X_train, Y_train, X_test, Y_test)\n",
        "    #GradientBoost(100, 1, X_train, Y_train, X_test, Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KYMRBV_y-P7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
        "\n",
        "    knn = KNeighborsClassifier(n_neighbors=7, weights='uniform', metric=\"chebyshev\")\n",
        "    #knn.fit(X_train, Y_train)\n",
        "\n",
        "    #y_true, y_pred = Y_test, knn.predict(X_test)\n",
        "    fourError(X, Y, knn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOgFpxAprPQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=9)\n",
        "#Worst model\n",
        "complement = ComplementNB()\n",
        "learning_rate = [0.0001, 0.001, 0.01, 0.1, 1 ,2 ,3 ,4, 5]\n",
        "\n",
        "result_ada_1 = []\n",
        "result_gradient_1 = []\n",
        "for i in learning_rate: #i -> Learning Rate\n",
        "    x = []\n",
        "    y = []\n",
        "    for j in range(50, 150): #j -> N estimators\n",
        "        x.append(AdaBoost(complement, j, i, X_train, Y_train, X_test, Y_test))\n",
        "        y.append(GradientBoost(j, i, X_train, Y_train, X_test, Y_test))\n",
        "    result_ada_1.append(x)\n",
        "    result_gradient_1.append(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYuojo9EKjFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
        "#best model\n",
        "clf = DecisionTreeClassifier(max_depth = 3)\n",
        "learning_rate = [0.0001, 0.001, 0.01, 0.1, 1 ,2 ,3 ,4, 5]\n",
        "\n",
        "result_ada_2 = []\n",
        "result_gradient_2 = []\n",
        "for i in learning_rate: #i -> Learning Rate\n",
        "    x = []\n",
        "    y = []\n",
        "    for j in range(50, 150): #j -> N estimators\n",
        "        x.append(AdaBoost(clf, j, i, X_train, Y_train, X_test, Y_test))\n",
        "        y.append(GradientBoost(j, i, X_train, Y_train, X_test, Y_test))\n",
        "    result_ada_2.append(x)\n",
        "    result_gradient_2.append(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sjzi8CsaNu2y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "251f6e9b-f97a-42f9-f88c-8ee3f857c446"
      },
      "source": [
        "from pandas.tools.plotting import parallel_coordinates\n",
        "# Perform parallel coordinate plot\n",
        "parallel_coordinates(result_ada_1, 'Class')\n",
        "plt.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-a58f637404fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparallel_coordinates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Perform parallel coordinate plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mparallel_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_ada_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Class'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas.tools'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQvMoKEPSjaF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parallel_coordinates(result_ada_2, 'Class')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxT-ikW4VMTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parallel_coordinates(result_gradient_1, 'Class')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnnzeXslVPWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parallel_coordinates(result_gradient_2, 'Class')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}